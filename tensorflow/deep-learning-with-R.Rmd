---
title: "deep-learning-with-R"
author: "yincy"
date: "10/24/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction  
```{r}
library(keras)
mnist <- dataset_mnist() 
train_images <- mnist$train$x 
train_labels <- mnist$train$y 
test_images <- mnist$test$x 
test_labels <- mnist$test$y
```


```{r}
network <- keras_model_sequential() %>% 
  layer_dense(units = 512, activation = "relu", input_shape = c(28*28)) %>% 
  layer_dense(units = 10, activation = "softmax")
```


```{r}
network %>% compile(
  optimizer = "rmsprop",
  loss = "categorical_crossentropy",
  metrics = c("accuracy")
)
```


```{r}
train_images <- array_reshape(train_images, dim = c(60000, 28*28))
train_images <- train_images / 255

test_images <- array_reshape(test_images, dim = c(10000, 28*28))
test_images <- test_images / 255
```


```{r}
train_labels <- to_categorical(train_labels)
test_labels <- to_categorical(test_labels)
```


```{r}
network %>% fit(train_images, train_labels, epochs = 5, batch_size = 128)
```


```{r}
metrics <- network %>% evaluate(test_images, test_labels)
```

```{r}
network %>% predict_classes(test_images[1:10, ])
```


```{r}
mnist <- dataset_mnist() 
train_images <- mnist$train$x 
train_labels <- mnist$train$y 
test_images <- mnist$test$x 
test_labels <- mnist$test$y
```


```{r}
par(mfrow = c(3, 3), mar = c(0,0,1,0))
digits <- train_images[1:9, , ]
for(i in 1:9){
    plot(as.raster(digits[i, , ], max = 255))
}
```

### Element-wise operations  
```{r}
x <- matrix(data = rnorm(100, 1, 1), 10, 10)
```

```{r}
naive_relu <- function(x){
  for(i in 1:nrow(x))
    for(j in 1:ncol(x))
      x[i, j] <- max(x[i, j], 0)
  x
}
```

```{r}
naive_relu(x)
```

```{r}
y <- matrix(data = rnorm(100, 10, 2), 10, 10)
```


```{r}
naive_add <- function(x, y){
  for(i in 1:nrow(x))
    for(j in 1:ncol(x))
      x[i, j] = x[i, j] + y[i, j]
  x
}
```

```{r}
naive_add(x, y)
```


### Operations involving tensors of different dimensions  
```{r}
?sweep
x <- matrix(1:100, 10,10)
y <- c(1:10)
```

```{r}
sweep(x = x, MARGIN = 2, STATS = y, FUN = "+")
```

MARGIN: 1 - row element wise operation; 2 - colulmn element wise operation  


```{r}
# x is a tensor of random values with shape (64, 3, 32, 10)
x <- array(round(runif(1000, 0, 9)), dim = c(64, 3, 32, 10))

# y is a tensor of 5s of shape (32, 10)
y <- array(data = 5, dim = c(32, 10))

# the output z has shape (64, 3, 32, 10) like x
z <- sweep(x = x, MARGIN = c(3, 4), STATS = y, FUN = pmax)
dim(z)
```


### tensor dot  
```{r}
x <- matrix(rnorm(16), 4, 4)
y <- matrix(rnorm(16), 4, 4)
x;y
```

```{r}
x %*% y
```

```{r}
x <- 1:5;y <- 6:10

naive_vector_dot <- function(x, y){
  z <- 0
  for(i in 1:length(x))
    z <- z + x[[i]] + y[[i]]
  z
}
```

```{r}
naive_vector_dot(x, y)
```

```{r}
naive_matrix_vector_dot <- function(x, y){
  z <- rep(0, nrow(x))
  for(i in 1:nrow(x))
    for(j in 1:ncol(x))
      z[[i]] <- z[[i]] + x[[i, j]] * y[[j]]
  z
}
```

```{r}
x <- matrix(1:12, 3, 4)
y <- 1:4

x;y
```

```{r}
naive_matrix_vector_dot(x, y)
```


```{rs}
x %*% y
```


```{r}
naive_matrix_vector_dot <- function(x, y){
  z <- rep(0, nrow(x))
  for(i in 1:nrow(x))
    z[[i]] <- naive_vector_dot(x[i, ], y)
  z
}
```

```{r}
naive_matrix_vector_dot(x, y)
```


### tensor reshaping  
```{r}
x <- matrix(c(0, 1,
              2, 3,
              4, 5),
            nrow = 3, ncol = 2, byrow = T)
```

```{r}
x <- array_reshape(x, dim = c(6,1))
```

```{r}
x <- array_reshape(x, dim = c(2, 3))
```
  
# Chapter 3 Getting started with neural networks  
```{r}
library(keras)
```

```{r}
imdb <- dataset_imdb(num_words = 10000)
c(c(train_data, train_labels), c(test_data, test_labels)) %<-% imdb 
```

```{r}
str(train_data[[1]]); train_labels[[1]]
```

```{r}
sapply(train_data, max) %>% max()
```

```{r}
word_index <- dataset_imdb_word_index()

reverse_word_index <- names(word_index)
names(reverse_word_index) <- word_index
```


```{r}
decode_review_index <- sapply(train_data[[1]], function(index){
  word <- if(index >= 3) reverse_word_index[[as.character(index-3)]]
  if(!is.null(word)) word else "?"
})
```

```{r}
vectorize_sequences <- function(sequences, dimension = 10000){
  results <- matrix(0, nrow = length(sequences), ncol = dimension)
  for(i in 1:length(sequences))
    results[i, sequences[[i]]] <- 1
  results
}
```


```{r}
x_train <- vectorize_sequences(train_data)
x_test <- vectorize_sequences(test_data)
```


## Classifying newswires: a multiclass classification example  
```{r}
library(keras)

retuers <- dataset_reuters(num_words = 10000)
c(c(train_data, train_labels), c(test_data, test_labels)) %<-% retuers 
```

```{r}
length(train_data);length(train_labels);length(test_data);length(test_labels)
```


```{r}
train_data[[2]] %>% length()
```


```{r}
word_index <- dataset_reuters_word_index()
word_index %>% head()
```


```{r}
reverse_word_index <- names(word_index)
names(reverse_word_index) <- word_index

reverse_word_index %>% head()
```

```{r}
data.frame(index = as.numeric(names(reverse_word_index)),
           word = reverse_word_index) %>% 
  dplyr::arrange(index)
```



```{r}
decoded_newswire <- sapply(train_data[[1]], function(index){
  word <- if(index >= 3) reverse_word_index[as.character(index - 3)]
  if(!is.null(word)) word else "?"
})
```


```{r}
vectorize_sequences <- function(sequences, dimension = 10000){ # 10000 words
  results <- matrix(0, nrow = length(sequences), ncol = dimension)
  for(i in 1:length(sequences))
    results[i, sequences[[i]]] <- 1
  results
}
```


```{r}
x_train <- vectorize_sequences(train_data)
x_test <- vectorize_sequences(test_data)
dim(x_train);dim(x_test)
```

```{r}
to_one_hot <- function(labels, dimension = 46){
  results <- matrix(0, nrow = length(labels), ncol = dimension)
  for(i in 1:length(labels))
    results[i, labels[[i]]] <- 1
  results
}
```

```{r}
one_hot_train_labels <- to_one_hot(train_labels)
one_hot_test_labels <- to_one_hot(test_labels)
```

```{r}
one_hot_train_labels <- to_categorical(train_labels)
one_hot_test_labels <- to_categorical(test_labels)
```

```{r}
model <- keras_model_sequential() %>% 
  layer_dense(units = 64, activation = "relu", input_shape = c(10000)) %>% # dimensionality of the input not sample axis, in this case is the dimension of columns
  layer_dense(units = 64, activation = "relu") %>% 
  layer_dense(units = 46, activation = "softmax")
```

```{r}
model %>% 
  compile(
    optimizer = "rmsprop",
    loss = "categorical_crossentropy",
    metrics = c("accuracy")
  )
```


```{r}
val_indices <- 1:1000
x_val <- x_train[val_indices, ]
partial_x_train <- x_train[-val_indices, ]

y_val <- one_hot_test_labels[val_indices, ]
partial_y_train <- one_hot_train_labels[-val_indices, ]
```


```{r}
histry <- model %>% 
  fit(
    partial_x_train,
    partial_y_train,
    epochs = 20,
    batch_size = 512,
    validation_data = list(x_val, y_val)
  )
```



```{r}
plot(histry)
```


```{r}
predictions <- model %>% 
  predict(x_test)

dim(predictions)
```

```{r}
classes <- apply(predictions, 1, which.max)
```


### a different way to handle the labels and loss  
```{r}
model %>% 
  compile(
    optimizer = "rmsprop",
    loss = "sparse_categorical_crossentropy",
    metrics  = c("accuracy")
  )
```
this new loss function is still mathematically the same as `categorical_crossentropy`; it just has a different interface.  

### the importance of having sufficiently large intermediate layers  
We mentioned earlier that because the final outputs are 46-dimensional, you should avoid intermediate layers with many fewer than 46 hidden units.  

```{r}
model <- keras_model_sequential() %>% 
  layer_dense(units = 64, activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 4, activation = "relu") %>% 
  layer_dense(units = 46, activation = "softmax")
```

```{r}
model %>% 
  compile(
    optimizer = "rmsprop",
    loss = "categorical_crossentropy",
    metrics = c("accuracy")
  )
```


```{r}
history <- model %>% 
  fit(
    partial_x_train,
    partial_y_train,
    epochs = 20,
    batch_size = 128,
    validation_data = list(x_val, y_val)
  )
```


```{r}
plot(history)
```



## Predicting house prices: a regression example  
The two previous examples considered classification problems, where the goal was to predict a single discrete label of an input data point. Another common type of machine-learning problem is regression , which consists of predicting a continuous value instead of a discrete label: for instance, predicting the temperature tomorrow, given meteorological data; or predicting the time that a software project will take to complete, given its specifications.  

> Don't confuse *regression* and the algorithm *logistic regression*. Confusingly, logistic regression isn't a regression algorithm -- it's a classification algorithm.  

### the bosten housing price dataset  
```{r}
library(keras)

dataset <- dataset_boston_housing()
c(c(train_data, train_targets), c(test_data, test_targets)) %<-% dataset 
```


```{r}
str(train_data);str(test_data)
```


### preparing the data  
It would be problematic to feed into a neural network values that all take wildly different ranges. The network might be able to automatically adapt to such heterogeneous data, but it would definitely make learning more difficult. A widespread best practice to deal with such data is to do feature-wise normalization.  

```{r}
mean <- apply(train_data, 2, mean)
sd <- apply(train_data, 2, sd)

train_data_s <- scale(train_data, center = mean, scale = sd)
test_data_s <- scale(test_data, center = mean, scale = sd)
```
you should never use in your workflow any quantity computed on the test data, even for something as simple as data normalization.  



### building the network  
Because so few samples are available, you will use a very small network with 2 hidden layers, each with 64 units.  

In general, the less training data you have, the worse overfitting will be, and using a small network is one way to migate overfitting.  

```{r}
huild_model <- function(){
  model <- keras_model_sequential() %>% 
    layer_dense(units = 64, activation = "relu", input_shape = dim(train_data)[[2]]) %>% 
    layer_dense(units = 64, activation = "relu") %>% 
    layer_dense(units = 1)
  
  model %>% 
    compile(
      optimizer = "rmsprop",
      loss = "mse",
      metrics = c("mae")
    )
}
```

The network ends with a single unit and no activation (it will be a linear layer).


### Validating your approach using k-fold validation  
```{r}
k <- 4

indices <- sample(1:nrow(train_data))
folds <- cut(indices, breaks = k, labels = F)

num_epochs <- 100
all_scores <- c()

for(i in 1:k){
  cat("processsing fold #", i, "\n")
  
  val_indices <- which(folds == i, arr.ind = T)
  val_data <- train_data[val_indices, ]
  val_targets <- train_targets[val_indices]
  
  partial_train_data <- train_data[-val_indices, ]
  partial_train_targets <- train_targets[-val_indices]
  
  model <- huild_model()
  
  model %>% 
    fit(
      partial_train_data,
      partial_train_targets,
      epochs = num_epochs,
      batch_size = 1, 
      verbose = 0)
  
  results <- model %>% 
    evaluate(val_data, val_targets, verbose = 0)
  
  all_scores <- c(all_scores, results$mae)
}
```

```{r}
all_scores; mean(all_scores)
```


# Chapter 4 Fundamentals of machine learning  






































