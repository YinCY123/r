---
title: "deep-learning-with-R"
author: "yincy"
date: "10/24/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction  
```{r}
library(keras)
mnist <- dataset_mnist() 
train_images <- mnist$train$x 
train_labels <- mnist$train$y 
test_images <- mnist$test$x 
test_labels <- mnist$test$y
```


```{r}
network <- keras_model_sequential() %>% 
  layer_dense(units = 512, activation = "relu", input_shape = c(28*28)) %>% 
  layer_dense(units = 10, activation = "softmax")
```


```{r}
network %>% compile(
  optimizer = "rmsprop",
  loss = "categorical_crossentropy",
  metrics = c("accuracy")
)
```


```{r}
train_images <- array_reshape(train_images, dim = c(60000, 28*28))
train_images <- train_images / 255

test_images <- array_reshape(test_images, dim = c(10000, 28*28))
test_images <- test_images / 255
```


```{r}
train_labels <- to_categorical(train_labels)
test_labels <- to_categorical(test_labels)
```


```{r}
network %>% fit(train_images, train_labels, epochs = 5, batch_size = 128)
```


```{r}
metrics <- network %>% evaluate(test_images, test_labels)
```

```{r}
network %>% predict_classes(test_images[1:10, ])
```


```{r}
mnist <- dataset_mnist() 
train_images <- mnist$train$x 
train_labels <- mnist$train$y 
test_images <- mnist$test$x 
test_labels <- mnist$test$y
```


```{r}
par(mfrow = c(3, 3), mar = c(0,0,1,0))
digits <- train_images[1:9, , ]
for(i in 1:9){
    plot(as.raster(digits[i, , ], max = 255))
}
```

### Element-wise operations  
```{r}
x <- matrix(data = rnorm(100, 1, 1), 10, 10)
```

```{r}
naive_relu <- function(x){
  for(i in 1:nrow(x))
    for(j in 1:ncol(x))
      x[i, j] <- max(x[i, j], 0)
  x
}
```

```{r}
naive_relu(x)
```

```{r}
y <- matrix(data = rnorm(100, 10, 2), 10, 10)
```


```{r}
naive_add <- function(x, y){
  for(i in 1:nrow(x))
    for(j in 1:ncol(x))
      x[i, j] = x[i, j] + y[i, j]
  x
}
```

```{r}
naive_add(x, y)
```


### Operations involving tensors of different dimensions  
```{r}
?sweep
x <- matrix(1:100, 10,10)
y <- c(1:10)
```

```{r}
sweep(x = x, MARGIN = 2, STATS = y, FUN = "+")
```

MARGIN: 1 - row element wise operation; 2 - colulmn element wise operation  


```{r}
# x is a tensor of random values with shape (64, 3, 32, 10)
x <- array(round(runif(1000, 0, 9)), dim = c(64, 3, 32, 10))

# y is a tensor of 5s of shape (32, 10)
y <- array(data = 5, dim = c(32, 10))

# the output z has shape (64, 3, 32, 10) like x
z <- sweep(x = x, MARGIN = c(3, 4), STATS = y, FUN = pmax)
dim(z)
```


### tensor dot  
```{r}
x <- matrix(rnorm(16), 4, 4)
y <- matrix(rnorm(16), 4, 4)
x;y
```

```{r}
x %*% y
```

```{r}
x <- 1:5;y <- 6:10

naive_vector_dot <- function(x, y){
  z <- 0
  for(i in 1:length(x))
    z <- z + x[[i]] + y[[i]]
  z
}
```

```{r}
naive_vector_dot(x, y)
```

```{r}
naive_matrix_vector_dot <- function(x, y){
  z <- rep(0, nrow(x))
  for(i in 1:nrow(x))
    for(j in 1:ncol(x))
      z[[i]] <- z[[i]] + x[[i, j]] * y[[j]]
  z
}
```

```{r}
x <- matrix(1:12, 3, 4)
y <- 1:4

x;y
```

```{r}
naive_matrix_vector_dot(x, y)
```


```{rs}
x %*% y
```


```{r}
naive_matrix_vector_dot <- function(x, y){
  z <- rep(0, nrow(x))
  for(i in 1:nrow(x))
    z[[i]] <- naive_vector_dot(x[i, ], y)
  z
}
```

```{r}
naive_matrix_vector_dot(x, y)
```


### tensor reshaping  
```{r}
x <- matrix(c(0, 1,
              2, 3,
              4, 5),
            nrow = 3, ncol = 2, byrow = T)
```

```{r}
x <- array_reshape(x, dim = c(6,1))
```

```{r}
x <- array_reshape(x, dim = c(2, 3))
```
  
# Chapter 3 Getting started with neural networks  
```{r}
library(keras)
```

```{r}
imdb <- dataset_imdb(num_words = 10000)
c(c(train_data, train_labels), c(test_data, test_labels)) %<-% imdb 
```

```{r}
str(train_data[[1]]); train_labels[[1]]
```

```{r}
sapply(train_data, max) %>% max()
```

```{r}
word_index <- dataset_imdb_word_index()

reverse_word_index <- names(word_index)
names(reverse_word_index) <- word_index
```


```{r}
decode_review_index <- sapply(train_data[[1]], function(index){
  word <- if(index >= 3) reverse_word_index[[as.character(index-3)]]
  if(!is.null(word)) word else "?"
})
```

```{r}
vectorize_sequences <- function(sequences, dimension = 10000){
  results <- matrix(0, nrow = length(sequences), ncol = dimension)
  for(i in 1:length(sequences))
    results[i, sequences[[i]]] <- 1
  results
}
```


```{r}
x_train <- vectorize_sequences(train_data)
x_test <- vectorize_sequences(test_data)
```


## Classifying newswires: a multiclass classification example  
```{r}
library(keras)

retuers <- dataset_reuters(num_words = 10000)
c(c(train_data, train_labels), c(test_data, test_labels)) %<-% retuers 
```

```{r}
length(train_data);length(train_labels);length(test_data);length(test_labels)
```


```{r}
train_data[[1]]
```


```{r}
word_index <- dataset_reuters_word_index()
word_index %>% head()
```


```{r}
reverse_word_index <- names(word_index)
names(reverse_word_index) <- word_index

reverse_word_index %>% head()
```

```{r}
data.frame(index = as.numeric(names(reverse_word_index)),
           word = reverse_word_index) %>% 
  dplyr::arrange(index)
```



```{r}
decoded_newswire <- sapply(train_data[[1]], function(index){
  word <- if(index >= 3) reverse_word_index[as.character(index - 3)]
  if(!is.null(word)) word else "?"
})
```


```{r}
vectorize_sequences <- function(sequences, dimension = 10000){
  results <- matrix(0, nrow = length(sequences), ncol = dimension)
  for(i in 1:length(sequences))
    results[i, sequences[[i]]] <- 1
  results
}
```


```{r}
x_train <- vectorize_sequences(train_data)
x_test <- vectorize_sequences(test_data)
dim(x_train);dim(x_test)
```

```{r}
to_one_hot <- function(labels, dimension = 46){
  results <- matrix(0, nrow = length(labels), ncol = dimension)
  for(i in 1:length(labels))
    results[i, labels[[i]]] <- 1
  results
}
```

```{r}
one_hot_train_labels <- to_one_hot(train_labels)
one_hot_test_labels <- to_one_hot(test_labels)
```

```{r}
one_hot_train_labels <- to_categorical(train_labels)
one_hot_test_labels <- to_categorical(test_labels)
```

```{r}
model <- keras_model_sequential() %>% 
  layer_dense(units = 64, activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 64, activation = "relu") %>% 
  layer_dense(units = 46, activation = "softmax")
```

```{r}
model %>% 
  compile(
    optimizer = "rmsprop",
    loss = "categorical_crossentropy",
    metrics = c("accuracy")
  )
```


```{r}
val_indices <- 1:1000
x_val <- x_train[val_indices, ]
partial_x_train <- x_train[-val_indices, ]

y_val <- one_hot_test_labels[val_indices, ]
partial_y_train <- one_hot_train_labels[-val_indices, ]
```


```{r}
histry <- model %>% 
  fit(
    partial_x_train,
    partial_y_train,
    epochs = 20,
    batch_size = 512,
    validation_data = list(x_val, y_val)
  )
```
























