---
title: "R Statistics"
author: "yincy"
date: "12/31/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


This tutorial comes from [this site](https://www.tutorialspoint.com/r/index.htm)

```{r}
library(magrittr)
```

# R Tutorial
## Data types
Generally, while doing programming in any programming language, you need to use various variables to store various information. **Variables are nothing but reserved memory locations to store values**. This means that, when you create a variable you reserve some space in memory.  

You may like to store information of various data types like character, wide character, integer, floating point, double floating point, Boolean etc. Based on the data type of a variable, the operating system allocates memory and decides what can be stored in the reserved memory.  

In contrast to other programming languages like C and java in R, the variables are not declared as some data type. The variables are assigned with R-Objects and the data type of the R-object becomes the data type of the variable. There are many types of R-objects. The frequently used ones are:  

- vector  
- list  
- matrix  
- array  
- factor  
- dataframe  


# Statistics Example  
## Mean, Median and Mode  
### Mean  
```{r}
x <- c(12, 7, 3, 4.2, 18, 2, 54, -21, 8, -5)
mean(x, trim = 0.1, na.rm = T)
```

**Applying Trim Option**  
When trim parameter is supplied, the values in the vector get sorted and then the required numbers of observations are dropped from calculating the mean.  

When trim = 0.3 (**30% of each end**), 3 values from each end will be dropped from the calculations to find mean.  

In this case the sorted vector is (−21, −5, 2, 3, 4.2, 7, 8, 12, 18, 54) and the values removed from the vector for calculating mean are (−21,−5,2) from left and (12,18,54) from right.  

```{r}
mean(x, trim = 0.3)
```

**Applying NA Option**  
If there are missing values, then the mean function returns NA.  

To drop the missing values from the calculation use na.rm = TRUE. which means remove the NA values.  

```{r}
x <- c(12,7,3,4.2,18,2,54,-21,8,-5,NA)

mean(x)

mean(x, na.rm = T)
```

### Median  
The middle most value in a data series is called the median.  

```{r}
x <- c(12,7,3,4.2,18,2,54,-21,8,-5)
median(x)
```


### Mode  
The mode is the value that has highest number of occurrences in a set of data. Unlike `mean` and `median`, `mode` can have both numeric and character data.  

R does not have a standard in-built function to calculate mode. So we create a user function to calculate mode of a data set in R. This function takes the vector as input and gives the mode value as output.  

```{r}
getmode <- function(v){
    suppressMessages(require(magrittr))
    uniqv <- unique(v)
    # uniqv[which.max(tabulate(match(v, uniqv)))]
    match(v, uniqv) %>% table() %>% which.max() %>% uniqv[.]
}

v <- c(2,1,2,3,1,2,3,4,1,5,5,3,2,3)
getmode(v)

charv <- c("o","it","the","it","it")
getmode(charv)
```


## Linear Regression  
Regression analysis is a very widely used statistical tool to establish a relationship model between two variables. One of these variable is called predictor variable whose value is gathered through experiments. The other variable is called response variable whose value is derived from the predictor variable.  

In Linear Regression these two variables are related through an equation, where exponent (power) of both these variables is 1. Mathematically a linear relationship represents a straight line when plotted as a graph. A non-linear relationship where the exponent of any variable is not equal to 1 creates a curve.  

The general mathematical equation for a linear regression is −  

$$y = ax + b$$

- **y** is the response variable.  
- **x** is the predictor variable.  
- **a** and **b** are constants which are called the coefficients.  

### Steps to Establish a Regression  
A simple example of regression is predicting weight of a person when his height is known. To do this we need to have the relationship between height and weight of a person.  

The steps to create the relationship is −  

- Carry out the experiment of gathering a sample of observed values of height and corresponding weight.  
- Create a relationship model using the `lm()` functions in R.  
- Find the coefficients from the mode created and create the mathematical equation using these  
- Get a summary of the relationship model to know the average error in prediction. Also called **residuals**.  
- To predict the weight of new persons, use the **predict()** function in R.  

```{r}
height <- c(151, 174, 138, 186, 128, 136, 179, 163, 152, 131)
weight <- c(63, 81, 56, 91, 47, 57, 76, 72, 62, 48)
```


```
lm(formula, data)
```

Following is the description of the parameters used -  

- **formula** is a symbol presenting the relationship between x and y.  
- **data** is the vector on which the formula will be applied.  

```{r}
relation <- lm(height ~ weight) 
# weight is the predictor variable, height is the response variable

relation
```

```{r}
library(sloop)
relation %>% class()

methods(class = "lm")
s3_methods_class("lm")
```


```{r}
summary(relation)
```


```{r}
coef(relation)
```

```{r}
residuals(relation)
plot(relation)
?plot.lm
```

**predict() function**  
```
predict(object, newdata)
```

- **object** is the formula which is already created using `lm()` function.  
- **newdata** is the vector containing the new value for predictor variable.  

```{r}
a <- data.frame(weight = 80)
predict(relation, a)
```


```{r}
plot(weight, height, pch = 19, col = "blue")
abline(coef = coef(relation), col = "grey", lty = 2)
```


## Multiple Regresssion  
Multiple regression is an extension of linear regression into relationship between more than two variables. In simple linear relation we have one predictor and one response variable, but in multiple regression we have more than one predictor variable and one response variable.  

The general mathematical equation for multiple regression is -  

$$y = a + b_{1}x_{1} + b_{2}x_{2} + ... + b_{n}x_{n}~$$


- **y** is the response variable  
- **a**, **b**, ..., **b~n~** are the coefficients  
- **x1**, **x2**, ..., **x~n~** are the predictor variables  

We create the regression model using the `lm()` function in R. The model determines the value of the coefficients using the input data. Next we can predict the value of the response variable for a given set of predictor variables using these coefficients.  

**lm() function**
```
lm(y ~ x1 + x2 + ..., data)
```

```{r}
input <- mtcars[, c("mpg", "disp", "hp", "wt")]

input %>% head()
```

```{r}
model <- lm(mpg ~ disp + hp + wt, data = input)

model %>% summary
```

```{r}
coef(model)
```

**prediction**    
```{r}
df <- data.frame(disp = 221, hp = 102, wt = 2.91)

predict(model, df)
```


## Logistic Regression  
The Logistic Regression is a regression model in which the response variable (dependent variable) has categorical values such as True/False or 0/1. **It actually measures the probability of a binary response as the value of response variable based on the mathematical equation relating it with the predictor variables**.  

The general mathematical equation for logistic regression is -  

$$y = \frac{1}{(1 + e^-(a + b_{1}x_{1} + b_{2}x_{2} + b_{3}x_{3} + ...)}$$

- **y** is the response variable  
- **x** is the predictor variable  
- **a** and **b** are the coefficients which are numeric constants.   

The function used to create the regression model is the **glm()** function.  

```
glm(formula, data, family)
```

- **formula** is the symbol presenting the relationship between the variables.  
- **data** is the data set giving the value of these variables.  
- **family** is R object to specify the details of the model. It's value is binomial for logistic regression.  

```{r}
input <- mtcars[, c("am", "cyl", "hp", "wt")]

input %>% head()
```

```{r}
am.data <- glm(formula = am ~ cyl + hp + wt, data = input, family = binomial())

am.data %>% summary()
```

```{r}
set.seed(111)
df <- data.frame(cyl = sample(c(4, 6, 8), 10, replace = T), 
                 hp = runif(n = 10, min = 90, max = 120), 
                 wt = runif(n = 10, min = 2.3, max = 3.5))


predict(am.data, newdata = df) %>% 
    as.matrix()
```


**conclusion**  
In the summary as the p-value in the last column is more than 0.05 for the variables "cyl" and "hp", we consider them to be insignificant in contributing to the value of the variable "am". Only weight (wt) impacts the "am" value in this regression model.   


## Normal Distribution  
In a random collection of data from independent sources, it is generally observed that the distribution of data is normal. Which means, on plotting a graph with the value of the variable in the horizontal axis and the count of the values in the vertical axis we get a bell shape curve. The center of the curve represents the mean of the data set. In the graph, fifty percent of values lie to the left of the mean and the other fifty percent lie to the right of the graph. This is referred as normal distribution in statistics.  

R built-in normal functions  

```
dnorm(x, mean, sd)
pnorm(x, mean, sd)
qnorm(p, mean, sd)
rnorm(n, mean, sd)
```

- **x** is a vector of numbers.  
- **p** is a vector of probabilities.  
- **n** is number of observations (sample size).  
- **mean** is the mean value of the sample data. It's default value is zero.  
- **sd** is the standard deviation. It's default value is 1.  

### dnorm  
This function gives height of the probability distribution at each point for a given mean and standard deviation.  

```{r}
x <- seq(-5, 5, by = 0.1)
y <- dnorm(x, mean = 0, sd = 1)

plot(x, y, type = "b", cex = 0.7, pch = 19)
curve(dnorm, from = -5, to = 5, add = T)
```

### pnorm  
This function gives the probability of a normally distributed random number to be less that the value of a given number. It is also called "Cumulative Distribution Function".   

```{r}
x <- seq(-10, 10, 0.2)
y <- pnorm(x, mean = 2.5, sd = 2)

f <- function(x, mean = 2.5, sd = 2){
    dnorm(x = x, mean = mean, sd = sd)
}

plot(x, y, pch = 19, type = "l")
curve(f, from = -10, to = 10, add = T)
```

### qnorm  
This function **takes the probability value and gives a number whose cumulative value matches the probability value**.  

```{r}
x <- seq(0, 1, by = 0.01)
y <- qnorm(x, mean = 0, sd = 1)

plot(x, y, pch = 19, cex = 0.5)
```

### rnorm  
This function is **used to generate random numbers whose distribution is normal**. It takes the sample size as input and generates that many random numbers.  

```{r}
y <- rnorm(n = 1000)
hist(y, breaks = 20)
```


## Binomial Distribution  
The binomial distribution model deals with finding the probability of success of an event which has only two possible outcomes in a series of experiments. For example, tossing of a coin always gives a head or a tail. The probability of finding exactly 3 heads in tossing a coin repeatedly for 10 times is estimated during the binomial distribution.  

R has four in-built functions to generate binomial distribution:  

```
dbinom(x, size, prob)
pbinom(x, size, prob)
qbinom(p, size, prob)
rbinom(n, size, prob)
```

- **x** is a vector of numbers.  
- **p** is a vector of probabilities.  
- **n** is number of observations.  
- **size** is the number of trials.  
- **prob** is the probability of success of each trial.  

### dbinom  
This function gives the probability density distribution at each point.  

```{r}
x <- seq(0, 50, 1)
y <- dbinom(x, size = 50, prob = 0.5)

plot(x, y, pch = 19, type = "b")
abline(v = 25)
```

### pbinom  
This function gives the cumulative probability of an event. It is a single value representing the probability.  

```{r}
# Probability of getting 26 or less heads from a 51 tosses of a coin.
q <-  seq(1, 51, 1)
x <- pbinom(q, size = 51, prob = 0.5)
plot(q, x, pch = 19)
# abline(v = 50)
```

### qbinom  
This function takes the probability value and gives a number whose cumulative value matches the probability value.  

```{r}
# How many heads will have a probability of 0.25 will come out when a coin
# is tossed 51 times.  

x <- qbinom(p = 0.25, size = 51, prob = 0.5)
x
```


### rbinom  
This function generates required number of random values of given probability from a given sample.  

```{r}
# Find 8 random values from a sample of 150 with probability of 0.4.  

x <- rbinom(8, 150, 0.4)
x
```


## Poisson Regression  
**Poisson Regression involves regression models in which the response variable is in the form of counts and not fractional numbers**. For example, the count of number of births or number of wins in a football match series. Also the values of the response variables follow a Poisson distribution.  

The general mathematical equation for Poisson regression is  

$$log(y) = a + b_{1}x_{1} + b_{2}x_{2} + ... + b_{}nx_{n}$$


- **y** is the response variable.  
- **a** and **b** are the numeric coefficients.  
- **x** is the predictor variable.  

The function used to create the poisson regression model is the **glm()** function.  

```
glm(formula, data, family)
```

- **formula** is the symbol presenting the relationship between the variables.  
- **data** is the data set giving the value of these variables.  
- **family** is R object to specify the details of the model. It's value is 'Poisson' for Logistic Regression.  


**Example**  
We have the in-built data set "warpbreaks" which describes the effect of wool type (A or B) and tension (low, medium or high) on the number of warp breaks per loom. Let's consider "breaks" as the response variable which is a count of number of breaks. The wool "type" and "tension" are taken as predictor variables.  

```{r}
input <- warpbreaks
input %>% head()

input %>% dplyr::pull(wool) %>% unique()
input %>% dplyr::pull(tension) %>% unique()
```

```{r}
output <- glm(formula = breaks ~ wool + tension, 
              data = warpbreaks, 
              family = "poisson")
```

```{r}
df <- data.frame(wool = sample(c("A", "B"), 10, replace = T), 
                 tension = sample(c("L", "M", "H"), 10, replace = T))

predict.glm(output, df)
```


```{r}
summary(output)
```

In the summary we look for the p-value in the last column to be less than 0.05 to consider an impact of the predictor variable on the response variable. As seen the wooltype B having tension type M and H have impact on the count of breaks.  


## Analysis of Covariance  
We use Regression analysis to create models which describe the effect of variation in predictor variables on the response variable. Sometimes, if we have a categorical variable with values like Yes/No or Male/Female etc. The simple regression analysis gives multiple results for each value of the categorical variable. In such scenario, we can study the effect of the categorical variable by using it along with the predictor variable and comparing the regression lines for each level of the categorical variable. Such an analysis is termed as **Analysis of Covariance** also called as **ANCOVA**.  

**Example**  
Consider the R built in data set mtcars. In it we observer that the field "am" represents the type of transmission (auto or manual). It is a categorical variable with values 0 and 1. The miles per gallon value(mpg) of a car can also depend on it besides the value of horse power("hp").  

We study the effect of the value of "am" on the regression between "mpg" and "hp". It is done by using the `aov()` function followed by the `anova()` function to compare the multiple regressions.  

```{r}
input <- mtcars[, c("am", "mpg", "hp")]
input %>% head()
```


**ANCOVA Analysis**  
We create a regression model taking "hp" as the predictor variable and "mpg" as the response variable taking into account the interaction between "am" and "hp".  

Model with interaction between categorical variable and predictor variable  
```{r}
result <- aov(formula = mpg ~ hp * am, data = input)
summary(result)
```

This result shows that both horse power and transmission type has significant effect on miles per gallon as the p value in both cases is less than 0.05. But the interaction between these two variables is not significant as the p-value is more than 0.05.  


Model without interaction between categorical variable and predictor variable.  
```{r}
input <- mtcars

result <- aov(mpg ~ hp + am, data = input)
summary(result)
```

This result shows that both horse power and transmission type has significant effect on miles per gallon as the p value in both cases is less than 0.05.  

**Comparing Two Models**  
Now we can **compare the two models to conclude if the interaction of the variables is truly in-significant**. For this we use the `anova()` function.  

```{r}
input <- mtcars

result1 <- aov(mpg ~ hp * am, data = input)
result2 <- aov(mpg ~ hp + am, data = input)

# compare the two models  
anova(result1, result2)
```

As the p-value is greater than 0.05 we conclude that the interaction between horse power and transmission type is not significant. So the mile age per gallon will depend in a similar manner on the horse power of the car in both auto and manual transmission mode.  


## Time Series Analysis  
Time series is a series of data points in which each data point is associated with a timestamp. A simple example is the price of a stock in the stock market at different points of time on a given day. Another example is the amount of rainfall in a region at different months of the year. R language uses many functions to create, manipulate and plot the time series data. The data for the time series is stored in an R object called time-series object. It is also a R data object like a vector or data frame.  

The time series object is created by using the `ts()` function.  

```
ts(data, start, end, frequency)
```

- **data** is a vector or matrix containing the value used in the time series.  
- **start** specifies the start time for the first observation in time series.  
- **end** specifies the end time for the last observation in time series.  
- **frequency** specifies the number of observations per unit time.  

**Example**  
Consider the annual rainfall details at a place starting from January 2012. We create an R time series object for a period of 12 months and plot it.  

```{r}
rainfall <- c(799,1174.8,865.1,1334.6,635.4,918.5,685.5,998.6,784.2,985,882.8,1071)

# convert it to a time series data.  
rainfall.timeseries <- ts(data = rainfall, 
                         start = c(2012, 1), 
                         frequency = 12)

plot(rainfall.timeseries)
```


**Different Time intervals**  
The value of the frequency parameter in the `ts()` function decides the time intervals at which the data points are measured. A value of 12 indicates that the time series is for 12 months. Other values and its meaning is as below  

- **frequency=12** peges the data points for every moth of a year.  
- **frequency=4** peges the data points for every quarter of a year.  
- **frequency=6** peges the data points every 10 minutes of an hour.  
- **frequency=24 x 6** peges the data points for every 10 minutes of a day.  


**Multiple Time Series**  
We can plot multiple time series in one chart by combinin both the series into a matrix.  

```{r}
rainfall1 <- c(799,1174.8,865.1,1334.6,635.4,918.5,685.5,998.6,784.2,985,882.8,1071)
rainfall2 <- c(655,1306.9,1323.4,1172.2,562.2,824,822.4,1265.5,799.6,1105.6,1106.7,1337.8)

# convert to matrix  
combined.rainfall <- matrix(c(rainfall1, rainfall2), nrow = 12, byrow = F)

# convert to time series 
rainfall.timeseries <- ts(combined.rainfall, start = c(2012, 1), frequency = 12)


plot(rainfall.timeseries, main = "Multiple Time Series")
```


## Nonlinear Least Square  
When modeling real world data for regression analysis, we observe that it is rarely the case that the equation of the model is a linear equation giving a linear graph. Most of the time, the equation of the model of real world data involves mathematical functions of higher degree like an exponent of 3 or a sin function. In such a scenario, the plot of the model gives a curve rather than a line. The goal of both linear and non-linear regression is to adjust the values of the model's parameters to find the line or curve that comes closest to your data. On finding these values we will be able to estimate the response variable with good accuracy.  

In Least Square regression, we establish a regression model in which the sum of the squares of the vertical distances of different points from the regression curve is minimized. We generally start with a defined model and assume some values for the coefficients. We then apply the `nls()` function of R to get the more accurate values along with the confidence intervals.  

```
nls(formula, data, start)
```

- **formula** is a nonlinear model formula including variables and parameters.  
- **data** is a data frame used to evaluate the variables in the formula.  
- **start** is a named list or named numeric vector of starting estimates.  


**Examples**  
We will consider a nonlinear model with assumption of initial values of its coefficients. Next we will see what is the confidence intervals of these assumed values so that we can judge how well these values fir into the model.  

a = b1 * x^2^ + b2

Let's assume the initial coefficients to be 1 and 3 and fit these values into nls() function.  

```{r}
x <- c(1.6,2.1,2,2.23,3.71,3.25,3.4,3.86,1.19,2.21)
y <- c(5.19,7.43,6.94,8.11,18.75,14.88,16.06,19.12,3.21,7.58)

model <- nls(y ~ b1 * x ^ 2 + b2 , 
             start = list(b1 = 1, b2 = 3))


df <- data.frame(
    x = seq(min(x), max(x), len = 100)
)
```

```{r}
plot(x, y)
lines(df$x, predict(model, df))
```

get the sum of the squared residuals.  
```{r}
sum(residuals(model)^2)
```


get the confidence intervals on the chosen values of the coefficients.  
```{r}
confint(model)
```

We can conclude that the value of b1 is more close to 1 while the value of b2 is more close to 2 and not 3.  

```{r}
summary(model)
```


## Decison Tree  
Decision tree is a graph to represent choices and their results in form of a tree. The nodes in the graph represent an event or choice and the edges of the graph represent the decision rules or conditions. It is mostly used in Machine Learning and Data Mining applications using R.  

Examples of use of decision tress is − predicting an email as spam or not spam, predicting of a tumor is cancerous or predicting a loan as a good or bad credit risk based on the factors in each of these. Generally, a model is created with observed data also called training data. Then a set of validation data is used to verify and improve the model. R has packages which are used to create and visualize decision trees. For new set of predictor variable, we use this model to arrive at a decision on the category (yes/No, spam/not spam) of the data.  

The R package `party`'s `ctree()` function is used to create decision trees

```{r, message=FALSE}
library(party)
```

```
ctree(formula, data)
```

- **formula** is a formula describing the predictor and response variables.  
- **data** is the name of the data set used.  

We will use the R in-built data set named **readingSkills** to create a decision tree. It describes the score of someone's readingSkills if we know the variables "age","shoesize","score" and whether the person is a native speaker or not.  

```{r}
readingSkills %>% head()

input <- readingSkills[1:105, ]

# create the tree
output.tree <- ctree(
    formula = nativeSpeaker ~ age + shoeSize + score, 
    data = input
)

plot(output.tree)
```

## Random Forest  
In the random forest approach, a large number of decision trees are created. Every observation is fed into every decision tree. The most common outcome for each observation is used as the final output. A new observation is fed into all the trees and taking a majority vote for each classification model.  

An error estimate is made for the cases which were not used while building the tree. That is called an **OOB (Out-of-bag)** error estimate which is mentioned as a percentage.  

The R package **"randomForest"** is used to create random forests.  

```{r, message=FALSE}
library(randomForest)
library(party)
```

The function `randomForest()` which is used to create and analyze random forest.  

```
randomForest(formula, data)
```

- **formula** is a formula describing the predictor and response variables.  
- **data** is the name of the data set used.  

We will use the R in-built data set named readingSkills to create a decision tree. It describes the score of someone's readingSkills if we know the variables "age","shoesize","score" and whether the person is a native speaker.  

```{r}
readingSkills %>% head
```


**Example**  
```{r}
output.forest <- randomForest(nativeSpeaker ~ age + shoeSize + score, 
                              data = readingSkills)

output.forest
```

```{r}
importance(output.forest, type = 2)
```

From the random forest shown above we can conclude that the shoesize and score are the important factors deciding if someone is a native speaker or not. Also the model has only 1% error which means we can predict with 99% accuracy.  


## Survival Analysis  
Survival analysis deals with predicting the time when a specific event is going to occur. It is also known as failure time analysis or analysis of time to death. For example predicting the number of days a person with cancer will survive or predicting the time when a mechanical system is going to fail.

The R package named `survival` is used to carry out survival analysis. This package contains the function `Surv()` which takes the input data as a R formula and creates a survival object among the chosen variables for analysis. Then we use the function `survfit()` to create a plot for the analysis.  

```{r}
library(survival)
```

```
Surv(time, event)
survfit(formula)
```

- **time** is the follow up time until the event occurs.  
- **event** indicates the status of occurrence of the expected event.  
- **formula** is the relationship between the predictor variables.  

**Example** 
We will consider the data set named "pbc" present in the survival packages installed above. It describes the survival data points about people affected with primary biliary cirrhosis (PBC) of the liver. Among the many columns present in the data set we are primarily concerned with the fields "time" and "status". Time represents the number of days between registration of the patient and earlier of the event between the patient receiving a liver transplant or death of the patient.  

```{r}
pbc %>% head
```

**Applying Surv() and survfit() function**  

```{r}
survfit(Surv(pbc$time, pbc$stage == 2) ~ 1)

plot(survfit(Surv(pbc$time, pbc$stage == 2) ~ 1))
```

The trend in the above graph helps us predicting the probability of survival at the end of a certain number of days.  

## Chi Square Test  
**Chi-Square test** is a statistical method to determine if two categorical variables have a significant correlation between them. Both those variables should be from same population and they should be categorical like − Yes/No, Male/Female, Red/Green etc.  

For example, we can build a data set with observations on people's ice-cream buying pattern and try to correlate the gender of a person with the flavor of the ice-cream they prefer. If a correlation is found we can plan for appropriate stock of flavors by knowing the number of gender of people visiting.  

```
chisq.test(data)
```

- **data** is the data in form of a table containing the count value of the variables in the observation.  

**Example**  
We will take the Cars93 data in the "MASS" library which represents the sales of different models of car in the year 1993.  

```{r}
library(MASS)
str(Cars93)
```

The above result shows the dataset has many Factor variables which can be considered as categorical variables. For our model we will consider the variables "AirBags" and "Type". Here we aim to find out any significant correlation between the types of car sold and the type of Air bags it has. If correlation is observed we can estimate which types of cars can sell better with what types of air bags.  

```{r}
car.data <- data.frame(Cars93$AirBags, Cars93$Type)
car.data <- car.data %>% table()
chisq.test(car.data)
```

The result shows the p-value of less than 0.05 which indicates a string correlation.   





