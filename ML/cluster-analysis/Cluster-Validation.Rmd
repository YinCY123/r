---
title: "Cluster-Validation"
author: "yincy"
date: "10/17/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Clustering Validation

The **cluster validation** consists of measuring the goodness of clustering results. Before applying any clustering algorithm to a data set, the first thing to do is to assess the *clustering tendency*. That is, whether applying clustering is suitable for the data. If yes, then how many clusters are there. Next, you can perform hierarchical clustering or partitioning clustering (with a pre-specified number of clusters). Finally, you can use a number of measures, describe in this part, to evaluate the goodness of the clustering results.  

- Assessing clustering tendency  
- Determining the optimal number of clusters  
- Cluster validation statistics  
- Computing p-value for hierachical clustering  


## Assessing Clustering Tendency
Before applying any clustering method on your data, itâ€™s important to evaluate whether the data sets contains meaningful clusters (i.e.: non-random structures) or not. If yes, then how many clusters are there. This process is defined as the assessing of **clustering tendency** or the feasibility of the clustering analysis.  

A big issue, in cluster analysis, is that clustering methods will return clusters even if the data does not contain any clusters. In other words, if you blindly apply a clustering method on a data set, it will divide the data into clusters because that is what it supposed to do.  

```{r}
library(factoextra)
library(clustertend)
```


```{r}
data(iris)
iris %>% head

df <- iris[, -5]
random_df <- apply(df, 2, 
                   function(x){
                       runif(length(x), min(x), max(x))
                   })

random_df <- as.data.frame(random_df)

df <- iris.scaled <- scale(df)
random_df <- scale(random_df)
```


```{r}
km.res1 <- kmeans(df, 3)
fviz_cluster(list(data = df, cluster = km.res1$cluster), 
             ellipse.type = "norm", 
             geom = "point", 
             stand = F, 
             palette = "jco", 
             ggtheme = theme_classic())
```


```{r}
km.res2 <- kmeans(random_df, 3)
fviz_cluster(object = list(data = random_df, cluster = km.res2$cluster), 
             ellipse.type = "t", 
             geom = "point", 
             stand = F, 
             palette = "jco", 
             ggtheme = theme_classic())

fviz_dend(hclust(dist(random_df)), 
          k = 3, 
          k_colors = "jco", 
          as.ggplot = T, 
          show_labels = F)
```


It can be seen that the k-means algorithm and the hierarchical clustering impose a classification on the random uniformly distributed data set even if there are no meaningful clusters present in it. This is way clustering tendency assessment should be used to evaluate the validity of clustering analysis.  

## Methods for assessing clustering tendency
### Statisitcal methods
The Hopkins statistic is used to assess the clustering tendency of a data set by measuring the probability that a given data set is generated by a uniform data distribution. In other words, it tests the spatial randomness of the data.  

**Calculate Hopkins statistc**  

1. Sample uniformly $n$ points $(p_{1}, ..., p_{n})$ from D.  

2. For each point $p_{i} \in D$, find it's nearest neighbor $p_{j}$; then compute the distance between $p_{i}$ and $p_{j}$ and denote it as $x_{i} = dist(p_{i}, p_{j})$.  

3. Generate a simulated data set ($random_{D}$) drawn from a random uniform distribution with $n$ points ($q_{1}, q_{2}, ..., q_{n}$) and the same variation as the original real data set D.  

4. For each data point $q_{i} \in random_{D}$, find it's mearest neighbor $q_{j}$ in D; then compute the distance between $q_{i}$ and $q_{j}$ and denote it $y_{i} = dist(q_{i}, q_{j})$.  

5. Calculate the Hopkins statistic (H) as the mean nearest neighbor distance in the random data set divided by the sum of the mean nearest neighbor distances in the real and across the simulated data set.  

$$
H = \frac{\sum_{i=1}^{n}y_{i}}{\sum_{i=1}^{n}x_{i} + \sum_{i=1}^{n}y_{i}}
$$

A value of H about 0.5 means that $\sum_{i = 1}^{n}y_{i}$ and $\sum_{i=1}^{n}x_{i}$ are close to each other, and thus the data D is uniformly distributed.  

If the value of H is close to zero, than the data set D is significantly a clusterable data.  

```{r}
library(clustertend)
hopkins(data = df, n = nrow(df) - 1)
```

```{r}
hopkins(data = random_df, n = nrow(random_df) - 1)
```

### Visual methods
The algorithm of visual assessment of cluster tendency.  

1. Compute the dissimilarity (DM) matrix between the objects in the data set using the Euclidean distance measure.  

2. Reorder the DM so that similar objects are close to one another. This process create an ordered dissimilarity matrix (ODM).  

3. The ODM is a displayed as an ordered dissimilarity image (ODI), which is the visual output of VAT.  

```{r}
fviz_dist(dist.obj = dist(df), show_labels = F) +
    labs(title = "Iris data")
```

```{r}
fviz_dist(dist.obj = dist(random_df), show_labels = F) +
    labs(title = "Random data")
```






