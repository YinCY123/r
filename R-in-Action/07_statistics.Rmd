---
title: "07 statistics"
author: "yincy"
date: "3/11/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Descriptive statistics
```{r}
myvars <- c("mpg", "hp", "wt")
# head(mtcars[, myvars])

summary(mtcars[, myvars])
```

```{r}
options(digits = 3)
mystats <- function(x, na.omit = F){
    if(na.omit){
        x <- x[!is.na(x)]
    }
    m = mean(x)
    n = length(x)
    s = sd(x)
    skew = sum((x - m)^3 / s^3) / n
    kurt = sum((x - m)^4/s^4)/ n - 3
    return(c(n = n, mean = m, stdev = s, skew = skew, kurtosis = kurt))
}

myvars <- c("mpg", "hp", "wt")
sapply(mtcars[, myvars], mystats, na.omit = T)
```

**packages for descriptive analysis**:  
- `Hmisc`  
- `pastecs`  
- `psych`  
- `skimr`  
- `summarytools`  


```{r}
library(Hmisc)

myvars <- c("mpg", "hp", "wt")
describe(mtcars[, myvars])
```

### group wised description
group by one variable  

```{r}
dstats <- function(x)sapply(x, mystats)
myvars <- c("mpg", "hp", "wt")
by(data = mtcars[, myvars], 
   INDICES = mtcars$am, 
   FUN = dstats)
```

group by two variables
```{r}
dstats <- function(x)sapply(x, mystats, na.omit = TRUE)
myvars <- c("mpg", "hp", "wt")
by(data = mtcars[, myvars], 
   INDICES = list(Transmission = mtcars$am, 
                  Engine = mtcars$vs), 
   FUN = dstats)
```

```{r}
library(carData)
library(dplyr)
library(magrittr)
data("Salaries")

Salaries %>% 
    summarise(med = median(salary), 
          min = min(salary), 
          max = max(salary))
```


```{r, warning=FALSE}
Salaries %>% 
    group_by(rank, sex) %>% 
    summarise(n = length(salary), 
              med = median(salary), 
              min = min(salary), 
              max = max(salary))
```

```{r}
starwars %>% 
    summarise_at(.vars = c("height", "mass"), .funs = mean, na.rm = T)

starwars %>% 
    summarise_if(.predicate = is.numeric, .fun = summary, na.rm = T)

starwars[, c(2, 3)] %>% 
    summarise_all(.funs = list(mean = mean, std = sd), na.rm = T)
```


## Frequency and contactingency tables
```{r}
library(vcd)
library(magrittr)
data(Arthritis)

Arthritis %>% head
```


|**Function**|**Description**|
|:--|:--|
|table(var1, var2, ..., varN)|Create an N-way contingency table from N categorical variables (factors).|
|xtable(formula, data)|Create N-way contingency table based on a formula and a matrix or a data frame|
|prop.table(table, margins)|Expresses table entries as fractions of the marginal table defined by the margins|
|margin.table(table, margins)|Computes the sum of table entries for a marginal table defined by margins|
|addmargins(table, margins)|Puts summary margins(sums by default) on a table|
|ftable(table)|Create a compact, 'flat' contingency table.|


### One-way tables
```{r}
options(digits = 3)
mytable <- with(Arthritis, table(Improved))
mytable
mytable %>% prop.table()
mytable %>% prop.table() * 100
```

### Two-way tables
```{r}
options(digits = 3)
mytable <- xtabs(~ Treatment + Improved, data = Arthritis)
mytable 
mytable %>% margin.table(margin = 1)
mytable %>% margin.table(margin = 2)

mytable %>% prop.table(margin = 1) # proportion of the row
mytable %>% prop.table(margin = 2) # proportion of the column

mytable %>% prop.table() %>% addmargins()
mytable %>% prop.table() %>% addmargins(margin = 1)
mytable %>% prop.table() %>% addmargins(margin = 2)
```


```{r}
library(gmodels)

with(Arthritis, CrossTable(Treatment, Improved, prop.chisq = T))
```


## Multidimensional tables
Both `table()` and `xtabs()` can be used to generate multidimensional tables based on three or more categorical variables. The `margin.table()`, `prop.table()`, and `addmargins()` functions extend naturally to more than two dimensions. Additionally, the `ftable()` function can be used to print multidimensional tables in a compact and attractive manner.  


```{r}
mytable <- xtabs(~ Treatment + Sex + Improved, data = Arthritis)

mytable
mytable %>% margin.table(1)
mytable %>% margin.table(2)
mytable %>% margin.table(3)

mytable %>% ftable()
mytable %>% ftable()

mytable %>% prop.table() %>% addmargins() %>% ftable() * 100
```


### Tests of independence
Test the independence of categorical variables.  
- chi-square test  
- Fisher exact test  
- Cochran-Mantel-Haenszel test  

**chi-square test**
```{r}
options(digits = 4)
library(vcd)

mytable <- xtabs(~ Treatment + Improved, data = Arthritis)
chisq.test(mytable)

mytable <- xtabs(~ Sex + Improved, data = Arthritis)
chisq.test(mytable)

# the warning message is produced because one of the six cells in the table
# has an expected value less then five, which may invalidate the chi-square
# approximation.
```

**fisher exact test**
Fisher’s exact test evaluates the null hypothesis of independence of rows and columns in a contingency table with fixed marginals.  

In contrast to many statistical packages, the fisher.test() function can be applied to any two-way table with two or more rows and columns, not just a 2 × 2 table.  

```{r}
mytable <- xtabs(~ Treatment + Improved, data = Arthritis)
mytable %>% fisher.test()
```

**Cochran-Mantel-Haenszel test**
The `mantelhaen.test()` function provides a Cochran–Mantel–Haenszel chi-square test of the null hypothesis that two nominal variables are conditionally independent in each stratum of a third variable.  
```{r}
mytable <- xtabs(~ Treatment + Sex + Improved, data = Arthritis)
mantelhaen.test(mytable)
```

### Measures of association
```{r}
library(vcd)

mytable <- xtabs(~ Treatment + Improved, data = Arthritis)
mytable %>% assocstats()
```


# Correlations
Correlation coefficients are used to describe relationships among quantitative variables. The sign (plus or minus) indicates the direction of the relationship (positive or inverse), and the magnitude indicates the strength of the relationship (ranging from 0 for no relationship to 1 for a perfectly predictable relationship).  

## Types of correlations
- Pearson: assesses the degree of linear relationship between two quantitative variables.  
- Spearman: rank-order correlation coefficient assesses the degree of relationship between two rank-ordered variables.  
- Kendall: a nonparametric measure of rank correlation.  

The `cor()` function produce all three correlation coefficients, where the `cov()` function provides covariances.  

**options of cor/cov**

|**Option**|**Description**|
|:--|:--|
|X|Matrix or data frame|
|Use|Specifies the handling of missing data.|
|Method|Specifies the type of correlation. The options are `pearson`,`spearman` and `kendall`.|


- partial: is a correlation between two quantitative variables, controlling for one or more other quantitative variables. `ggm` provides `pcor()` function to calculate partial correlation coefficients.  

```
pcor(u, s)
```
- u: is a vector of numbers, with the first two numbers being the indices of the variables to be correlated, and the remaining numbers being the indices of the conditioning variables (that is, the variables being partialed out).  
- s: is the covariance matrix among the variables.  


- polychoric: 
- polyserial  


```{r}
options(digits = 2)
states <- state.x77[, 1:6]
cov(states)
cor(states)
cor(states, method = "spearman")
```


```{r}
library(ggm)
colnames(states)
pcor(u = c(1, 5, 2, 3, 6), 
     S = cov(states))
```

The `hetcor()` function in the `polycor` package can compute a heterogeneous correlation matrix containing Pearson product-moment correlations between numeric variables, polyserial correlations between numeric and ordinal variables, polychoric correlations between ordinal variables, and tetrachoric correlations between two dichotomous variables. Polyserial, polychoric, and tetrachoric correlations assume that the ordinal or dichotomous variables are derived from underlying normal distributions.  


### Testing correlations for significance
```{r}
cor.test(states[, 3], states[, 5])
```

The `corr.test()` function provided in the `psych` package allows you to produce correlations and significance levels for matrices of Pearson, Spearman, and Kendall correlations.  

```{r}
options(digits = 5)
library(psych)

corr.test(states, use = "complete")
```

The `pcor.test()` function in the `psych` package can be used to test the conditional independence of two variables controlling for one or more additional variables, assuming multivariate normality.  

```
pcor.test(r, q, n)
```

- r: the partial correlation prodiced by the `pcor()`  
- q: the number of variables being controlled  
- n: is the sample size  



# T-tests
The groups compared are continuous and assumed to be distributed normally.  

## independent t-test
assumptions:  
1. continuous  
2. normally distributed  

```{r}
library(MASS)
data("UScrime")

# hypothesis: true difference in means between the two group is not equal to 0
t.test(Prob ~ So, 
       data = UScrime, 
       alternative = "two", 
       mu = 0, 
       var.equal = T)
```

### Dependent t-test
assumption:
1. the difference between groups is normally distributed  

```{r}
library(MASS)

sapply(UScrime[, c("U1", "U2")], function(x){c(mean = mean(x), sd = sd(x))})

with(UScrime, t.test(U1, U2, paired = T))
```



## Nonparametric tests of group differences
if the outcome variables are severely skewed or ordinal in nature, using nonparametric tests

### Comparing two groups
If the two groups are independent, you can use the Wilcoxon rank sum test (more popularly known as the Mann–Whitney U test) to assess whether the observations are sampled from the same probability distribution (that is, whether the probability of obtaining higher scores is greater in one population than the other).  

```{r}
with(UScrime, by(Prob, So, mean))
```

```{r}
wilcox.test(Prob ~ So, data = UScrime)
```

It’s appropriate in situations where the groups are paired and the assumption of normality is unwarranted.  


```{r}
sapply(UScrime[, c("U1", "U2")], median)

with(UScrime, wilcox.test(U1, U2, paired = T))
```


### Comparing more than two groups
If the groups are independent, a Kruskal–Wallis test provides a useful approach. If the groups are dependent (for example, repeated measures or randomized block design), the Friedman test is more appropriate.  

```{r}
states <- data.frame(state.region, state.x77)
```


```{r}
kruskal.test(Illiteracy ~ state.region, data = states)
```


```{r}
source("wmc.txt")
states <- data.frame(state.region, state.x77)

wmc(Illiteracy ~ state.region, data = states, method = "holm")
```





