---
title: "13_Generalized-linear-models"
author: "yincy"
date: "3/24/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


regression and ANOVA can be used to predict a **normally distributed** response variable from a set of continuous and/or categorical predictor variables. 

generalized linear models extend the linear-model framework to include dependent variables that are decidedly non-normal.  


**link function**  

|**Family**|**Default link function**|
|:--|:--|
|binomial|(link = 'logit')|
|gaussian|(link = 'identity')|
|gamma|(link = 'inverse')|
|inverse.gaussian|(link = '1/mu^2')|
|poisson|(link = 'log')|
|quasi|(link = 'identity', variance = 'constant')|
|quasibinomial|(link = 'logit')|
|quasipoisson|(link = 'log')|


Logistic regression is applied to situations in which the response variable is dichotomous (0 or 1). The model assumes that Y follows a binomal distribution and that you can fit a linear model of the form.  

$$\log_{e}(\frac{\pi}{1 - \pi}) = \beta_{0} + \sum_{j=1}^{p}\beta_{j}X_{j}$$


Poisson regression is applied to situations in which the response variable is the number of events to occur in a given period of time. The Poisson regression assumes that Y follows a Poisson distribution and that you can fit a linear model of the form.  
$$log_{e}(\lambda) = \beta_{0} + \sum_{j=1}^{p}\beta_{j}X_{j}$$

Where $\lambda$ is the mean (and variance) of Y.  

It's worth noting that the standard linear model is also a special case of the generalized linear model. If you let the link function $g(u_{Y}) = u_{Y}$ or the identity function and specify that the probability distribution is normal (Gaussian), then the following two form will produce the same result.  

```
glm(Y ~ X1 + X2 + X3, family = gaussian(link = "identity"), data = mydata)

lm(Y ~ X1 + X2 + X3, data = mydata)
```


## Logistic regression
```{r}
library(AER)
library(magrittr)
data("Affairs")

Affairs %>% summary()
Affairs$affairs %>% table()
```


```{r}
Affairs$affairs <- ifelse(Affairs$affairs > 0, 1, 0) 
Affairs$affairs <- factor(Affairs$affairs, levels = c(0, 1), labels = c("No", "Yes"))
Affairs$affairs %>% table
```

```{r}
fit <- glm(affairs ~ gender + age + yearsmarried + children + religiousness + education + occupation + rating, data = Affairs, family = binomial(link = "logit"))
fit %>% summary()
```

```{r}
fit1 <- glm(affairs ~ age + yearsmarried + religiousness + rating, 
            data = Affairs, 
            family = binomial())
fit1 %>% summary()
```

```{r}
anova(fit, fit1, test = "Chisq")
```

```{r}
options(digits = 2)
coef(fit1) %>% exp
confint(fit)
```

```{r}
testdata <- data.frame(
    rating = c(1, 2, 3, 4, 5), 
    age = mean(Affairs$age), 
    yearsmarried = mean(Affairs$yearsmarried), 
    religiousness = mean(Affairs$religiousness)
)

testdata
```

```{r}
testdata$pred <- predict(fit1, testdata, type = "response")
testdata
```


### Overdispersion
The expected variance for data drawn from a binomial distribution is  
$$\sigma = n\pi(1 - n)$$

where n is the number of observations and $\pi$ is the probability of belonging to the Y = 1 group. *Overdispersion* occurs when the observed variance of the response variable is larger than what would be expected from a binomial distribution. Overdispersion can lead to distorted test standard errors and inaccurate tests of significance.  

When overdispersion is present, you can still fit a logistic regression using the `glm()` function, but in this case, you should use the quasibinomial distribution rather than the binomial distribution. 


One way to detect overdispersion is to compare the residual deviance with the residual degrees of freedom in your binomial model. If the ratio $\phi = \frac{Residua \ deviance}{Residual \ df}$ is considerably large than 1, you have evidence of overdispersion.  


```{r}
deviance(fit1) / df.residual(fit1)
```

```{r}
fit <- glm(affairs ~ age + yearsmarried + religiousness + rating, family = binomial(), data = Affairs)
fit1 <- glm(affairs ~ age + yearsmarried + religiousness + rating, family = quasibinomial(), data = Affairs)

fit %>% summary
pchisq(summary(fit1)$dispersion * fit$df.residual, fit$df.residual, lower.tail = F)
```

- *Robust logistic regression* - The `glmRob()` function in the `robustbase` package can be used to fit a robust generalized linear model, including robust logistic regression. Robust logistic regression can be helpful when fitting logisitc regression models to data containing outliers and inflential observations.  

- *Multinomial logistic regression* - If the response variable has more than one unordered categories using the `mlogit()` function in the `mlogit` package. Alternatively, you can use the `multinom()` function in the `nnet` package.  

- *Ordinal logistic regression* - If the response variable is a set of ordered categories, you can fit an ordinal logistic regression using the `polyr()` function in the `MASS` package.  


```{r}
library(robustbase)
glmrob(affairs ~ age + rating + yearsmarried + religiousness, data = Affairs, family = binomial()) %>% summary()
```


## Poisson regression
Poisson regression is useful when youâ€™re predicting an outcome variable representing counts from a set of continuous and/or categorical predictor variables.  

```{r}
data(epilepsy, package = "robustbase")
library(magrittr)
epilepsy %>% names
epilepsy %>% summary()
```

```{r}
fit <- glm(Ysum ~ Base + Age + Trt, data = epilepsy, family = poisson())
fit %>% summary()
```









